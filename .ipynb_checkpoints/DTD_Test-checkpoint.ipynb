{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from models import scribbler, discriminator, texturegan, localDiscriminator\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import sys, os\n",
    "from skimage import color\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import visdom\n",
    "from IPython.display import display\n",
    "import torchvision.models as models\n",
    "from dataloader import imfol\n",
    "from torch.utils.data.sampler import SequentialSampler\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from dataloader.imfol import ImageFolder, make_dataset\n",
    "\n",
    "from utils import transforms as custom_transforms\n",
    "from utils.visualize import vis_patch, vis_image\n",
    "from models import scribbler, discriminator, define_G, weights_init, scribbler_dilate_128\n",
    "import argparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "def parse_arguments(argv):\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    ###############added options#######################################\n",
    "    parser.add_argument('-lr', '--learning_rate', default=1e-5, type=float,\n",
    "                        help='Learning rate for the generator')\n",
    "    parser.add_argument('-lrd', '--learning_rate_D', default=1e-4, type=float,\n",
    "                        help='Learning rate for the discriminator')\n",
    "\n",
    "    parser.add_argument('--gan', default='lsgan', type=str, choices=['dcgan', 'lsgan', 'wgan', 'improved wgan'],\n",
    "                        help='dcgan|lsgan|wgan|improved wgan')  # todo wgan/improved wgan\n",
    "\n",
    "    parser.add_argument('--model', default='scribbler', type=str, choices=['scribbler', 'pix2pix','scribbler_dilate_128'],\n",
    "                        help='scribbler|pix2pix')\n",
    "\n",
    "    parser.add_argument('--num_epoch', default=1, type=int,\n",
    "                        help='texture|scribbler')\n",
    "\n",
    "    parser.add_argument('--visualize_every', default=10, type=int,\n",
    "                        help='no. iteration to visualize the results')\n",
    "\n",
    "    # all the weights ratio, might wanna make them sum to one\n",
    "    parser.add_argument('--feature_weight', default=100, type=float,\n",
    "                        help='weight ratio for feature loss')\n",
    "    parser.add_argument('--pixel_weight_l', default=400, type=float,\n",
    "                        help='weight ratio for pixel loss for l channel')\n",
    "    parser.add_argument('--pixel_weight_ab', default=800, type=float,\n",
    "                        help='weight ratio for pixel loss for ab channel')\n",
    "    parser.add_argument('--pixel_weight_rgb', default=800, type=float,\n",
    "                        help='weight ratio for pixel loss for ab channel')\n",
    "\n",
    "    parser.add_argument('--discriminator_weight', default=2e1, type=float,\n",
    "                        help='weight ratio for the discriminator loss')\n",
    "    parser.add_argument('--style_weight', default=1, type=float,\n",
    "                        help='weight ratio for the texture loss')\n",
    "\n",
    "    # parser.add_argument('--gpu', default=[0], type=int, nargs='+',\n",
    "    #                     help='List of GPU IDs to use')  # TODO support cpu\n",
    "    parser.add_argument('--gpu', default=3, type=int, help=\"GPU ID\")\n",
    "\n",
    "    parser.add_argument('--display_port', default=7779, type=int,\n",
    "                        help='port for displaying on visdom (need to match with visdom currently open port)')\n",
    "\n",
    "    parser.add_argument('--data_path', default='/home/psangkloy3/training_shoes_pretrain/', type=str,\n",
    "                        help='path to the data directory, expect train_skg, train_img, val_skg, val_img')\n",
    "\n",
    "    parser.add_argument('--save_dir', default='/home/psangkloy3/shoes_best_models/', type=str,\n",
    "                        help='path to save the model')\n",
    "\n",
    "    parser.add_argument('--load_dir', default='/home/psangkloy3/shoes_best_models/', type=str,\n",
    "                        help='path to save the model')\n",
    "\n",
    "    parser.add_argument('--save_every', default=1000, type=int,\n",
    "                        help='no. iteration to save the models')\n",
    "\n",
    "    parser.add_argument('--load', default=0, type=int,\n",
    "                        help='load generator and discrminator from iteration n')\n",
    "    parser.add_argument('--load_D', default=0, type=int,\n",
    "                        help='load discriminator from iteration n, priority over load')\n",
    "\n",
    "    parser.add_argument('--image_size', default=128, type=int,\n",
    "                        help='Training images size, after cropping')\n",
    "    parser.add_argument('--resize_max', default=1, type=float,\n",
    "                        help='max resize, ratio of the original image, max value is 1')\n",
    "    parser.add_argument('--resize_min', default=0.6, type=float,\n",
    "                        help='min resize, ratio of the original image, min value 0')\n",
    "    parser.add_argument('--patch_size_min', default=20, type=int,\n",
    "                        help='minumum texture patch size')\n",
    "    parser.add_argument('--patch_size_max', default=40, type=int,\n",
    "                        help='max texture patch size')\n",
    "\n",
    "    parser.add_argument('--batch_size', default=32, type=int, help=\"Training batch size\")\n",
    "\n",
    "    parser.add_argument('--num_input_texture_patch', default=2,type=int)\n",
    "\n",
    "    parser.add_argument('--local_texture_size', default=50, type=int,\n",
    "                        help='use local texture loss instead of global, set -1 to use global')\n",
    "    parser.add_argument('--color_space', default='lab', type=str, choices=['lab', 'rgb'],\n",
    "                        help='lab|rgb')\n",
    "\n",
    "    parser.add_argument('--threshold_D_max', default=0.8, type=int,\n",
    "                        help='stop updating D when accuracy is over max')\n",
    "\n",
    "    parser.add_argument('--content_layers', default='relu4_2', type=str,\n",
    "                        help='Layer to attach content loss.')\n",
    "    parser.add_argument('--style_layers', default='relu3_2, relu4_2', type=str,\n",
    "                        help='Layer to attach content loss.')\n",
    "\n",
    "    parser.add_argument('--use_segmentation_patch', default=True, type=bool,\n",
    "                        help='whether or not to inject noise into the network')\n",
    "\n",
    "    parser.add_argument('--input_texture_patch', default='dtd_texture', type=str,\n",
    "                        choices=['original_image', 'dtd_texture'],\n",
    "                        help='whether or not to inject noise into the network')\n",
    "\n",
    "    ############################################################################\n",
    "    ############################################################################\n",
    "    ############TODO: TO ADD #################################################################\n",
    "    parser.add_argument('--tv_weight', default=1, type=float,\n",
    "                        help='weight ratio for total variation loss')\n",
    "\n",
    "    parser.add_argument('--mode', default='texture', type=str, choices=['texture', 'scribbler'],\n",
    "                        help='texture|scribbler')\n",
    "\n",
    "    parser.add_argument('--crop', default='random', type=str, choices=['random', 'center'],\n",
    "                        help='random|center')\n",
    "\n",
    "    parser.add_argument('--contrast', default=True, type=bool,\n",
    "                        help='randomly adjusting contrast on sketch')\n",
    "\n",
    "    parser.add_argument('--occlude', default=False, type=bool,\n",
    "                        help='randomly occlude part of the sketch')\n",
    "\n",
    "    parser.add_argument('--checkpoints_path', default='data/', type=str,\n",
    "                        help='output directory for results and models')\n",
    "\n",
    "    parser.add_argument('--noise_gen', default=False, type=bool,\n",
    "                        help='whether or not to inject noise into the network')\n",
    "\n",
    "    parser.add_argument('--absolute_load', default='', type=str,\n",
    "                        help='load saved generator model from absolute location')\n",
    "\n",
    "    ##################################################################################################################################\n",
    "\n",
    "    return parser.parse_args(argv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Val Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "command = '--display_port 7777 --load 0 --load_D 0 --gpu 3 --model scribbler --feature_weight 10 --pixel_weight_ab 5000 --pixel_weight_l 500 --style_weight 1000 --discriminator_weight 5000 --learning_rate 1e-3 --learning_rate_D 1e-4 --load_dir /home/wendy/scribbler_models_1_DTD_1 --save_dir /home/wendy/scribbler_models_1_DTD_1 --data_path /home/wendy/training_handbags_pretrain/ --batch_size 16 --save_every 500 --num_epoch 100000 --num_input_texture_patch 2 --image_size 256 --patch_size_min 40 --patch_size_max 60'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "args = parse_arguments(command.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if args.color_space == 'lab':\n",
    "            transform = custom_transforms.Compose([\n",
    "                custom_transforms.RandomSizedCrop(args.image_size,args.resize_min,args.resize_max),\n",
    "                custom_transforms.RandomHorizontalFlip(),\n",
    "                custom_transforms.toLAB(),\n",
    "                custom_transforms.toTensor()\n",
    "            ])\n",
    "\n",
    "elif args.color_space == 'rgb':\n",
    "            transform = custom_transforms.Compose([\n",
    "                custom_transforms.RandomSizedCrop(args.image_size,args.resize_min,args.resize_max),\n",
    "                custom_transforms.RandomHorizontalFlip(),\n",
    "                custom_transforms.toRGB('RGB'),\n",
    "                custom_transforms.toTensor()\n",
    "            ])\n",
    "            args.pixel_weight_ab = args.pixel_weight_rgb\n",
    "            args.pixel_weight_l = args.pixel_weight_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val = imfol.make_dataset(args.data_path, 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_network(model, network_label, epoch_label, gpu_id, save_dir):\n",
    "    save_filename = '%s_net_%s.pth' % (epoch_label, network_label)\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    save_path = os.path.join(save_dir, save_filename)\n",
    "    torch.save(model.cpu().state_dict(), save_path)\n",
    "    model.cuda(device_id=gpu_id)\n",
    "\n",
    "\n",
    "def load_network(model, network_label, epoch_label,save_dir):\n",
    "    save_filename = '%s_net_%s.pth' % (epoch_label, network_label)\n",
    "    save_path = os.path.join(save_dir, save_filename)\n",
    "    model.load_state_dict(torch.load(save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "valDset = ImageFolder('val', args.data_path, transform)\n",
    "val_display_size = args.batch_size\n",
    "indices = torch.randperm(len(valDset))\n",
    "val_display_sampler = SequentialSampler(indices[:val_display_size])\n",
    "valLoader = DataLoader(dataset=valDset, batch_size=val_display_size,sampler=val_display_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded G from itr:0\n",
      "Loaded D from itr:0\n"
     ]
    }
   ],
   "source": [
    "        if args.gan =='lsgan':\n",
    "            sigmoid_flag = 0 \n",
    "\n",
    "        if args.model == 'scribbler':\n",
    "            netG = scribbler.Scribbler(5, 3, 32)\n",
    "        elif args.model == 'texturegan':\n",
    "            netG = texturegan.TextureGAN(5, 3, 32)\n",
    "        elif args.model == 'pix2pix':\n",
    "            netG = define_G(5, 3, 32)\n",
    "        elif args.model == 'scribbler_dilate_128':\n",
    "            netG = scribbler_dilate_128.ScribblerDilate128(5, 3, 32)\n",
    "        else:\n",
    "            print(args.model + ' not support. Using Scribbler model')\n",
    "            netG = scribbler.Scribbler(5, 3, 32)\n",
    "\n",
    "        if args.color_space == 'lab':\n",
    "            netD = discriminator.Discriminator(1,32,sigmoid_flag)\n",
    "        elif args.color_space == 'rgb':\n",
    "            netD = discriminator.Discriminator(3,32,sigmoid_flag)\n",
    "        feat_model=models.vgg19(pretrained=True)\n",
    "        if args.load == -1:\n",
    "            netG.apply(weights_init)\n",
    "        else:\n",
    "\n",
    "            load_network(netG,'G',args.load,args.load_dir)\n",
    "            print('Loaded G from itr:' + str(args.load))\n",
    "        if args.load_D == -1:\n",
    "            netD.apply(weights_init)  \n",
    "        else:\n",
    "            load_network(netD,'D',args.load_D,args.load_dir)\n",
    "            print('Loaded D from itr:' + str(args.load_D))\n",
    "\n",
    "        if args.gan =='lsgan':\n",
    "            criterion_gan = nn.MSELoss()\n",
    "        elif args.gan =='dcgan':\n",
    "            criterion_gan = nn.BCELoss()\n",
    "        else:\n",
    "            raise Warning(\"Undefined GAN type. Defaulting to LSGAN\")\n",
    "            criterion_gan = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from train import gen_input, rand_between"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_input_exact(img, skg, seg, xcenter=64, ycenter=64, crop_size=60, num_patch=1):\n",
    "    #generate input skg with random patch from img\n",
    "    #input img,skg [bsx3xwxh], xcenter,ycenter, size \n",
    "    #output bsx5xwxh\n",
    "    MAX_COUNT = 10000\n",
    "    bs,c,w,h = img.size()\n",
    "    results = torch.Tensor(bs,5,w,h)\n",
    "    texture_info = []\n",
    "\n",
    "    #text_info.append([xcenter,ycenter,crop_size])    \n",
    "    seg = seg/torch.max(seg)\n",
    "    counter = 0\n",
    "    for i in range(bs):\n",
    "        counter=0\n",
    "        ini_texture = torch.ones(img[0].size())*(1)\n",
    "        ini_mask =  torch.ones((1,w,h))*(-1)\n",
    "        temp_info = []\n",
    "        for j in range(num_patch):\n",
    "            xstart = max(int(xcenter-crop_size/2), 0)\n",
    "            ystart = max(int(ycenter-crop_size/2), 0)\n",
    "            xend = min(int(xcenter + crop_size/2), w)\n",
    "            yend = min(int(ycenter + crop_size/2), h)\n",
    "            patch = seg[i, xstart:xend, ystart:yend]\n",
    "            sizem = torch.ones(patch.size())            \n",
    "\n",
    "            temp_info.append([xcenter, ycenter, crop_size])\n",
    "            res = gen_input(img[i], skg[i], ini_texture, ini_mask, xcenter, ycenter, crop_size)\n",
    "          \n",
    "            ini_texture = res[1:4,:,:]\n",
    "            \n",
    "        texture_info.append(temp_info)\n",
    "        results[i,:,:,:] = res\n",
    "    return results, texture_info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = valLoader.__iter__().__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (2) : out of memory at /opt/conda/conda-bld/pytorch_1503966894950/work/torch/lib/THC/generic/THCStorage.cu:66",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-c4f3ed27ef58>\u001b[0m in \u001b[0;36mmove_patch\u001b[0;34m(x, xcenter, ycenter, size)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mimg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mskg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mseg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda2/lib/python2.7/site-packages/torch/_utils.pyc\u001b[0m in \u001b[0;36m_cuda\u001b[0;34m(self, device, async)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mnew_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (2) : out of memory at /opt/conda/conda-bld/pytorch_1503966894950/work/torch/lib/THC/generic/THCStorage.cu:66"
     ]
    }
   ],
   "source": [
    "@interact(x=[0,25],xcenter=[0,128],ycenter=[0,128],size=[10,256])\n",
    "def move_patch(x,xcenter,ycenter,size):\n",
    "    img, skg, seg,txt = data \n",
    "    img=custom_transforms.normalize_lab(img)\n",
    "    skg=custom_transforms.normalize_lab(skg)\n",
    "    txt=custom_transforms.normalize_lab(txt)\n",
    "    \n",
    "    if not args.use_segmentation_patch:\n",
    "        seg.fill_(1)\n",
    "    if args.input_texture_patch == 'original_image':\n",
    "        inp, texture_loc = gen_input_exact(img, skg, seg[:,0,:,:], xcenter,ycenter,size)\n",
    "    elif args.input_texture_patch == 'dtd_texture':\n",
    "        inp, texture_loc = gen_input_exact(txt, skg, seg[:,0,:,:], xcenter,ycenter,size)\n",
    "                            \n",
    "                            \n",
    "    img=img.cuda()\n",
    "    skg=skg.cuda()\n",
    "    seg=seg.cuda()\n",
    "    txt = txt.cuda()\n",
    "    inp = inp.cuda()\n",
    "    \n",
    "    if args.input_texture_patch == 'original_image':\n",
    "        inp_img = vis_patch(custom_transforms.denormalize_lab(img.cpu()),\n",
    "                                            custom_transforms.denormalize_lab(skg.cpu()),\n",
    "                                            texture_loc,\n",
    "                                            args.color_space)\n",
    "    elif args.input_texture_patch == 'dtd_texture':\n",
    "        inp_img = vis_patch(custom_transforms.denormalize_lab(txt.cpu()),\n",
    "                                            custom_transforms.denormalize_lab(skg.cpu()),\n",
    "                                            texture_loc,\n",
    "                                            args.color_space)\n",
    "        tar_img = vis_image(custom_transforms.denormalize_lab(img.cpu()),\n",
    "                                            args.color_space)\n",
    "    plt.figure(1)    \n",
    "    plt.imshow(np.transpose(inp_img[x],(1, 2, 0)))\n",
    "    \n",
    "    input_stack = torch.FloatTensor()\n",
    "    input_stack = input_stack.cuda()\n",
    "    input_stack.resize_as_(inp.float()).copy_(inp)\n",
    "\n",
    "    inputv = Variable(input_stack)\n",
    "    netG.cuda()\n",
    "    outputG = netG(inputv)\n",
    "    \n",
    "    plt.figure(2)                    \n",
    "    out_img = vis_image(custom_transforms.denormalize_lab(outputG.data.double().cpu()), args.color_space)\n",
    "    plt.imshow(np.transpose(out_img[x],(1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
