{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from models import scribbler, discriminator, texturegan, localDiscriminator\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import sys, os\n",
    "from skimage import color\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import visdom\n",
    "from IPython.display import display\n",
    "import torchvision.models as models\n",
    "from dataloader import imfol\n",
    "from torch.utils.data.sampler import SequentialSampler\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from dataloader.imfol import ImageFolder, make_dataset\n",
    "\n",
    "from utils import transforms as custom_transforms\n",
    "from utils.visualize import vis_patch, vis_image\n",
    "from models import scribbler, discriminator, define_G, weights_init, scribbler_dilate_128\n",
    "import argparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "def parse_arguments(argv):\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    ###############added options#######################################\n",
    "    parser.add_argument('-lr', '--learning_rate', default=1e-3, type=float,\n",
    "                        help='Learning rate for the generator')\n",
    "    parser.add_argument('-lrd', '--learning_rate_D', default=1e-4, type=float,\n",
    "                        help='Learning rate for the discriminator')\n",
    "    parser.add_argument('-lrd_l', '--learning_rate_D_local', default=1e-4, type=float,\n",
    "                        help='Learning rate for the discriminator')\n",
    "\n",
    "    parser.add_argument('--gan', default='lsgan', type=str, choices=['dcgan', 'lsgan', 'wgan', 'improved wgan'],\n",
    "                        help='dcgan|lsgan|wgan|improved wgan')  # todo wgan/improved wgan\n",
    "\n",
    "    parser.add_argument('--model', default='scribbler', type=str, choices=['scribbler', 'texturegan', 'pix2pix','scribbler_dilate_128'],\n",
    "                        help='scribbler|pix2pix')\n",
    "\n",
    "    parser.add_argument('--num_epoch', default=100, type=int,\n",
    "                        help='texture|scribbler')\n",
    "\n",
    "    parser.add_argument('--visualize_every', default=10, type=int,\n",
    "                        help='no. iteration to visualize the results')\n",
    "\n",
    "    # all the weights ratio, might wanna make them sum to one\n",
    "    parser.add_argument('--feature_weight', default=0, type=float,\n",
    "                        help='weight ratio for feature loss')\n",
    "    parser.add_argument('--pixel_weight_l', default=400, type=float,\n",
    "                        help='weight ratio for pixel loss for l channel')\n",
    "    parser.add_argument('--pixel_weight_ab', default=0, type=float,\n",
    "                        help='weight ratio for pixel loss for ab channel')\n",
    "    parser.add_argument('--pixel_weight_rgb', default=0, type=float,\n",
    "                        help='weight ratio for pixel loss for ab channel')\n",
    "\n",
    "    parser.add_argument('--discriminator_weight', default=0, type=float,\n",
    "                        help='weight ratio for the discriminator loss')\n",
    "    parser.add_argument('--discriminator_local_weight', default=2e3, type=float,\n",
    "                        help='weight ratio for the discriminator loss')\n",
    "    parser.add_argument('--style_weight', default=1, type=float,\n",
    "                        help='weight ratio for the texture loss')\n",
    "\n",
    "    # parser.add_argument('--gpu', default=[0], type=int, nargs='+',\n",
    "    #                     help='List of GPU IDs to use')  # TODO support cpu\n",
    "    parser.add_argument('--gpu', default=1, type=int, help=\"GPU ID\")\n",
    "\n",
    "    parser.add_argument('--display_port', default=7779, type=int,\n",
    "                        help='port for displaying on visdom (need to match with visdom currently open port)')\n",
    "\n",
    "    parser.add_argument('--data_path', default='/home/psangkloy3/training_handbags_pretrain/', type=str,\n",
    "                        help='path to the data directory, expect train_skg, train_img, val_skg, val_img')\n",
    "\n",
    "    parser.add_argument('--save_dir', default='/home/psangkloy3/test/', type=str,\n",
    "                        help='path to save the model')\n",
    "\n",
    "    parser.add_argument('--load_dir', default='/home/psangkloy3/test/', type=str,\n",
    "                        help='path to save the model')\n",
    "\n",
    "    parser.add_argument('--save_every', default=1000, type=int,\n",
    "                        help='no. iteration to save the models')\n",
    "\n",
    "    parser.add_argument('--load_epoch', default=-1, type=int,\n",
    "                        help=\"The epoch number for the model to load\")\n",
    "    parser.add_argument('--load', default=-1, type=int,\n",
    "                        help='load generator and discrminator from iteration n')\n",
    "    parser.add_argument('--load_D', default=-1, type=int,\n",
    "                        help='load discriminator from iteration n, priority over load')\n",
    "\n",
    "    parser.add_argument('--image_size', default=128, type=int,\n",
    "                        help='Training images size, after cropping')\n",
    "    parser.add_argument('--resize_max', default=1, type=float,\n",
    "                        help='max resize, ratio of the original image, max value is 1')\n",
    "    parser.add_argument('--resize_min', default=0.6, type=float,\n",
    "                        help='min resize, ratio of the original image, min value 0')\n",
    "    parser.add_argument('--patch_size_min', default=20, type=int,\n",
    "                        help='minumum texture patch size')\n",
    "    parser.add_argument('--patch_size_max', default=40, type=int,\n",
    "                        help='max texture patch size')\n",
    "\n",
    "    parser.add_argument('--batch_size', default=32, type=int, help=\"Training batch size. MUST BE EVEN NUMBER\")\n",
    "\n",
    "    parser.add_argument('--num_input_texture_patch', default=2,type=int)\n",
    "    parser.add_argument('--num_local_texture_patch', default=1,type=int)\n",
    "\n",
    "    parser.add_argument('--color_space', default='lab', type=str, choices=['lab', 'rgb'],\n",
    "                        help='lab|rgb')\n",
    "\n",
    "    parser.add_argument('--threshold_D_max', default=0.8, type=int,\n",
    "                        help='stop updating D when accuracy is over max')\n",
    "\n",
    "    parser.add_argument('--content_layers', default='relu4_2', type=str,\n",
    "                        help='Layer to attach content loss.')\n",
    "    parser.add_argument('--style_layers', default='relu3_2, relu4_2', type=str,\n",
    "                        help='Layer to attach content loss.')\n",
    "\n",
    "    parser.add_argument('--use_segmentation_patch', default=True, type=bool,\n",
    "                        help='whether or not to inject noise into the network')\n",
    "\n",
    "    parser.add_argument('--input_texture_patch', default='dtd_texture', type=str,\n",
    "                        choices=['original_image', 'dtd_texture'],\n",
    "                        help='whether or not to inject noise into the network')\n",
    "    \n",
    "    parser.add_argument('--loss_texture', default='dtd_texture', type=str,\n",
    "                        choices=['original_image', 'dtd_texture'],\n",
    "                        help='where is the texture loss come from')\n",
    "    \n",
    "    parser.add_argument('--local_texture_size', default=50, type=int,\n",
    "                        help='use local texture loss instead of global, set -1 to use global')\n",
    "    \n",
    "    parser.add_argument('--texture_discrminator_loss', default=True, type=bool,\n",
    "                        help='adding discrminator for texture')\n",
    "    \n",
    "    ############################################################################\n",
    "    ############################################################################\n",
    "    ############TODO: TO ADD #################################################################\n",
    "    parser.add_argument('--tv_weight', default=1, type=float,\n",
    "                        help='weight ratio for total variation loss')\n",
    "\n",
    "    parser.add_argument('--mode', default='texture', type=str, choices=['texture', 'scribbler'],\n",
    "                        help='texture|scribbler')\n",
    "\n",
    "    parser.add_argument('--crop', default='random', type=str, choices=['random', 'center'],\n",
    "                        help='random|center')\n",
    "\n",
    "    parser.add_argument('--contrast', default=True, type=bool,\n",
    "                        help='randomly adjusting contrast on sketch')\n",
    "\n",
    "    parser.add_argument('--occlude', default=False, type=bool,\n",
    "                        help='randomly occlude part of the sketch')\n",
    "\n",
    "    parser.add_argument('--checkpoints_path', default='data/', type=str,\n",
    "                        help='output directory for results and models')\n",
    "\n",
    "    parser.add_argument('--noise_gen', default=False, type=bool,\n",
    "                        help='whether or not to inject noise into the network')\n",
    "\n",
    "    parser.add_argument('--absolute_load', default='', type=str,\n",
    "                        help='load saved generator model from absolute location')\n",
    "\n",
    "    ##################################################################################################################################\n",
    "\n",
    "    return parser.parse_args(argv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Val Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "command = '--display_port 7775 --load 0 --load_D 0 --load_epoch 129 --gpu 2 --model scribbler --feature_weight 10 --pixel_weight_ab 1e3 --pixel_weight_l 1e3 --style_weight 1 --discriminator_weight 1e2 --discriminator_local_weight 0 --learning_rate 1e-2 --learning_rate_D 1e-4 --load_dir /home/wendy/scribbler_models_handbags_DTD_0 --save_dir /home/wendy/scribbler_models_handbags_DTD_0 --data_path /home/wxian3/training_handbags_pretrain/ --batch_size 45 --save_every 500 --num_epoch 100000 --input_texture_patch dtd_texture --loss_texture dtd_texture --local_texture_size 50 --num_input_texture_patch 1 --num_local_texture_patch 2 --patch_size_min 30 --patch_size_min 50'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "args = parse_arguments(command.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from main import get_transforms\n",
    "from models import save_network, load_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transform = get_transforms(args)\n",
    "val = imfol.make_dataset(args.data_path, 'val')\n",
    "valDset = ImageFolder('val', args.data_path, transform)\n",
    "val_display_size = args.batch_size\n",
    "indices = torch.randperm(len(valDset))\n",
    "val_display_sampler = SequentialSampler(indices[:val_display_size])\n",
    "valLoader = DataLoader(dataset=valDset, batch_size=val_display_size,sampler=val_display_sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from main import get_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "netG, netD, netD_local = get_models(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from train import gen_input, rand_between, gen_input_rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_input_exact(img, skg, seg, xcenter=64, ycenter=64, crop_size=60, num_patch=1):\n",
    "    #generate input skg with random patch from img\n",
    "    #input img,skg [bsx3xwxh], xcenter,ycenter, size \n",
    "    #output bsx5xwxh\n",
    "    MAX_COUNT = 10000\n",
    "    bs,c,w,h = img.size()\n",
    "    results = torch.Tensor(bs,5,w,h)\n",
    "    texture_info = []\n",
    "\n",
    "    #text_info.append([xcenter,ycenter,crop_size])    \n",
    "    seg = seg/torch.max(seg)\n",
    "    counter = 0\n",
    "    for i in range(bs):\n",
    "        counter=0\n",
    "        ini_texture = torch.ones(img[0].size())*(1)\n",
    "        ini_mask =  torch.ones((1,w,h))*(-1)\n",
    "        temp_info = []\n",
    "        for j in range(num_patch):\n",
    "            xstart = max(int(xcenter-crop_size/2), 0)\n",
    "            ystart = max(int(ycenter-crop_size/2), 0)\n",
    "            xend = min(int(xcenter + crop_size/2), w)\n",
    "            yend = min(int(ycenter + crop_size/2), h)\n",
    "            patch = seg[i, xstart:xend, ystart:yend]\n",
    "            sizem = torch.ones(patch.size())            \n",
    "\n",
    "            temp_info.append([xcenter, ycenter, crop_size])\n",
    "            res = gen_input(img[i], skg[i], ini_texture, ini_mask, xcenter, ycenter, crop_size)\n",
    "          \n",
    "            ini_texture = res[1:4,:,:]\n",
    "            \n",
    "        texture_info.append(temp_info)\n",
    "        results[i,:,:,:] = res\n",
    "    return results, texture_info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = valLoader.__iter__().__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 3: sizes do not match at /opt/conda/conda-bld/pytorch_1503966894950/work/torch/lib/THC/generated/../generic/THCTensorMathPointwise.cu:217",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-152-ad9d5aa69fad>\u001b[0m in \u001b[0;36mmove_patch\u001b[0;34m(x, xcenter, ycenter, size)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mnetG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0moutputG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolor_space\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'lab'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wendy/texturegan/models/texturegan.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mskip_block\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda2/lib/python2.7/site-packages/torch/nn/modules/container.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wendy/texturegan/models/texturegan.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownsample\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresidual\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m             \u001b[0;32mprint\u001b[0m \u001b[0mresidual\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mresidual\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda2/lib/python2.7/site-packages/torch/autograd/variable.pyc\u001b[0m in \u001b[0;36m__iadd__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iadd__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 817\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__sub__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda2/lib/python2.7/site-packages/torch/autograd/variable.pyc\u001b[0m in \u001b[0;36madd_\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_sub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda2/lib/python2.7/site-packages/torch/autograd/variable.pyc\u001b[0m in \u001b[0;36m_add\u001b[0;34m(self, other, inplace)\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mAdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda2/lib/python2.7/site-packages/torch/autograd/_functions/basic_ops.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, a, b, inplace)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmark_dirty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 3: sizes do not match at /opt/conda/conda-bld/pytorch_1503966894950/work/torch/lib/THC/generated/../generic/THCTensorMathPointwise.cu:217"
     ]
    }
   ],
   "source": [
    "@interact(x=[0,25],xcenter=[0,128],ycenter=[0,128],size=[10,100])\n",
    "def move_patch(x,xcenter,ycenter,size):\n",
    "    img, skg, seg, eroded_seg, txt = data  # LAB with negeative value\n",
    "    with torch.cuda.device(args.gpu):\n",
    "        # this is in LAB value 0/100, -128/128 etc\n",
    "        img = custom_transforms.normalize_lab(img)\n",
    "        skg = custom_transforms.normalize_lab(skg)\n",
    "        txt = custom_transforms.normalize_lab(txt)\n",
    "        seg = custom_transforms.normalize_seg(seg)\n",
    "        eroded_seg = custom_transforms.normalize_seg(eroded_seg)\n",
    "        \n",
    "        bs, w, h = seg.size()\n",
    "\n",
    "        seg = seg.view(bs, 1, w, h)\n",
    "        seg = torch.cat((seg, seg, seg), 1)\n",
    "        \n",
    "        eroded_seg = eroded_seg.view(bs, 1, w, h)\n",
    "        eroded_seg = torch.cat((eroded_seg, eroded_seg, eroded_seg), 1)\n",
    "\n",
    "        temp = torch.ones(seg.size()) * (1 - seg).float()\n",
    "        temp[:, 1, :, :] = 0  # torch.ones(seg[:,1,:,:].size())*(1-seg[:,1,:,:]).float()\n",
    "        temp[:, 2, :, :] = 0  # torch.ones(seg[:,2,:,:].size())*(1-seg[:,2,:,:]).float()\n",
    "\n",
    "        txt = txt.float() * seg.float() + temp\n",
    "        # seg=custom_transforms.normalize_lab(seg)\n",
    "        # norm to 0-1 minus mean\n",
    "        if not args.use_segmentation_patch:\n",
    "            seg.fill_(1)\n",
    "            eroded_seg.fill_(1)\n",
    "        if args.input_texture_patch == 'original_image':\n",
    "            inp, texture_loc = gen_input_rand(img, skg, eroded_seg[:, 0, :, :] * 100,\n",
    "                                              args.patch_size_min, args.patch_size_max,\n",
    "                                              args.num_input_texture_patch)\n",
    "        elif args.input_texture_patch == 'dtd_texture':\n",
    "            inp, texture_loc = gen_input_rand(txt, skg, eroded_seg[:, 0, :, :] * 100,\n",
    "                                              args.patch_size_min, args.patch_size_max,\n",
    "                                              args.num_input_texture_patch)\n",
    "\n",
    "        img = img.cuda()\n",
    "        skg = skg.cuda()\n",
    "        seg = seg.cuda()\n",
    "        txt = txt.cuda()\n",
    "        inp = inp.cuda()\n",
    "        \n",
    "        input_stack = torch.FloatTensor().cuda()\n",
    "        input_stack.resize_as_(inp.float()).copy_(inp)\n",
    "        #target_img.resize_as_(img.float()).copy_(img)\n",
    "        #segment.resize_as_(seg.float()).copy_(seg)\n",
    "\n",
    "        inputv = Variable(input_stack)\n",
    "        #targetv = Variable(target_img)\n",
    "        \n",
    "        netG.cuda()\n",
    "        outputG = netG(inputv)\n",
    "\n",
    "    if args.color_space == 'lab':\n",
    "        out_img = vis_image(custom_transforms.denormalize_lab(outputG.data.double().cpu()),\n",
    "                            args.color_space)\n",
    "        if args.input_texture_patch == 'original_image':\n",
    "            inp_img = vis_patch(custom_transforms.denormalize_lab(img.cpu()),\n",
    "                                custom_transforms.denormalize_lab(skg.cpu()),\n",
    "                                texture_loc,\n",
    "                                args.color_space)\n",
    "        elif args.input_texture_patch == 'dtd_texture':\n",
    "            inp_img = vis_patch(custom_transforms.denormalize_lab(txt.cpu()),\n",
    "                                custom_transforms.denormalize_lab(skg.cpu()),\n",
    "                                texture_loc,\n",
    "                                args.color_space)\n",
    "        tar_img = vis_image(custom_transforms.denormalize_lab(img.cpu()),\n",
    "                            args.color_space)\n",
    "    elif args.color_space == 'rgb':\n",
    "\n",
    "        out_img = vis_image(custom_transforms.denormalize_rgb(outputG.data.double().cpu()),\n",
    "                            args.color_space)\n",
    "        inp_img = vis_patch(custom_transforms.denormalize_rgb(img.cpu()),\n",
    "                            custom_transforms.denormalize_rgb(skg.cpu()),\n",
    "                            texture_loc,\n",
    "                            args.color_space)\n",
    "        tar_img = vis_image(custom_transforms.denormalize_rgb(img.cpu()),\n",
    "                            args.color_space)\n",
    "        \n",
    "    plt.figure(1)    \n",
    "    plt.imshow(np.transpose(inp_img[x],(1, 2, 0)))\n",
    "    \n",
    "    plt.figure(2)                    \n",
    "    plt.imshow(np.transpose(out_img[x],(1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
