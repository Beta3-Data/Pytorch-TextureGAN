{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from models import scribbler, discriminator, texturegan, localDiscriminator\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import sys, os\n",
    "from skimage import color\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import visdom\n",
    "from IPython.display import display\n",
    "import torchvision.models as models\n",
    "from dataloader import imfol\n",
    "from torch.utils.data.sampler import SequentialSampler\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from dataloader.imfol import ImageFolder, make_dataset\n",
    "\n",
    "from utils import transforms as custom_transforms\n",
    "from utils.visualize import vis_patch, vis_image\n",
    "from models import scribbler, discriminator, define_G, weights_init, scribbler_dilate_128\n",
    "import argparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "def parse_arguments(argv):\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('-lr', '--learning_rate', default=1e-3, type=float,\n",
    "                        help='Learning rate for the generator')\n",
    "    parser.add_argument('-lrd', '--learning_rate_D', default=1e-4, type=float,\n",
    "                        help='Learning rate for the discriminator')\n",
    "    parser.add_argument('-lrd_l', '--learning_rate_D_local', default=1e-4, type=float,\n",
    "                        help='Learning rate for the discriminator')\n",
    "\n",
    "    parser.add_argument('--gan', default='lsgan', type=str, choices=['dcgan', 'lsgan', 'wgan', 'improved wgan'],\n",
    "                        help='dcgan|lsgan|wgan|improved wgan')  # todo wgan/improved wgan\n",
    "\n",
    "    parser.add_argument('--model', default='scribbler', type=str, choices=['scribbler', 'texturegan', 'pix2pix','scribbler_dilate_128'],\n",
    "                        help='scribbler|pix2pix')\n",
    "\n",
    "    parser.add_argument('--num_epoch', default=100, type=int,\n",
    "                        help='texture|scribbler')\n",
    "\n",
    "    parser.add_argument('--visualize_every', default=10, type=int,\n",
    "                        help='no. iteration to visualize the results')\n",
    "\n",
    "    # all the weights ratio, might wanna make them sum to one\n",
    "    parser.add_argument('--feature_weight', default=0, type=float,\n",
    "                        help='weight ratio for feature loss')\n",
    "    parser.add_argument('--global_pixel_weight_l', default=0, type=float,\n",
    "                        help='weight ratio for pixel loss for l channel')\n",
    "    parser.add_argument('--local_pixel_weight_l', default=1,  type=float,\n",
    "                        help='pixel weight for local loss patch')\n",
    "    parser.add_argument('--pixel_weight_ab', default=0, type=float,\n",
    "                        help='weight ratio for pixel loss for ab channel')\n",
    "    parser.add_argument('--pixel_weight_rgb', default=0, type=float,\n",
    "                        help='weight ratio for pixel loss for ab channel')\n",
    "\n",
    "    parser.add_argument('--discriminator_weight', default=0, type=float,\n",
    "                        help='weight ratio for the discriminator loss')\n",
    "    parser.add_argument('--discriminator_local_weight', default=0, type=float,\n",
    "                        help='weight ratio for the discriminator loss')\n",
    "    parser.add_argument('--style_weight', default=0, type=float,\n",
    "                        help='weight ratio for the texture loss')\n",
    "\n",
    "    parser.add_argument('--gpu', default=1, type=int, help=\"GPU ID\")\n",
    "\n",
    "    parser.add_argument('--display_port', default=7779, type=int,\n",
    "                        help='port for displaying on visdom (need to match with visdom currently open port)')\n",
    "\n",
    "    parser.add_argument('--data_path', default='/home/psangkloy3/training_handbags_pretrain/', type=str,\n",
    "                        help='path to the data directory, expect train_skg, train_img, val_skg, val_img')\n",
    "\n",
    "    parser.add_argument('--save_dir', default='/home/psangkloy3/test/', type=str,\n",
    "                        help='path to save the model')\n",
    "\n",
    "    parser.add_argument('--load_dir', default='/home/psangkloy3/test/', type=str,\n",
    "                        help='path to save the model')\n",
    "\n",
    "    parser.add_argument('--save_every', default=1000, type=int,\n",
    "                        help='no. iteration to save the models')\n",
    "\n",
    "    parser.add_argument('--load_epoch', default=-1, type=int,\n",
    "                        help=\"The epoch number for the model to load\")\n",
    "    parser.add_argument('--load', default=-1, type=int,\n",
    "                        help='load generator and discrminator from iteration n')\n",
    "    parser.add_argument('--load_D', default=-1, type=int,\n",
    "                        help='load discriminator from iteration n, priority over load')\n",
    "\n",
    "    parser.add_argument('--image_size', default=128, type=int,\n",
    "                        help='Training images size, after cropping')\n",
    "    parser.add_argument('--resize_to', default=300, type=int,\n",
    "                        help='Training images size, after cropping')\n",
    "                        \n",
    "    parser.add_argument('--resize_max', default=1, type=float,\n",
    "                        help='max resize, ratio of the original image, max value is 1')\n",
    "    parser.add_argument('--resize_min', default=0.6, type=float,\n",
    "                        help='min resize, ratio of the original image, min value 0')\n",
    "    parser.add_argument('--patch_size_min', default=20, type=int,\n",
    "                        help='minumum texture patch size')\n",
    "    parser.add_argument('--patch_size_max', default=40, type=int,\n",
    "                        help='max texture patch size')\n",
    "\n",
    "    parser.add_argument('--batch_size', default=32, type=int, help=\"Training batch size. MUST BE EVEN NUMBER\")\n",
    "\n",
    "    parser.add_argument('--num_input_texture_patch', default=2,type=int)\n",
    "    parser.add_argument('--num_local_texture_patch', default=1,type=int)\n",
    "\n",
    "    parser.add_argument('--color_space', default='lab', type=str, choices=['lab', 'rgb'],\n",
    "                        help='lab|rgb')\n",
    "\n",
    "    parser.add_argument('--threshold_D_max', default=0.8, type=int,\n",
    "                        help='stop updating D when accuracy is over max')\n",
    "\n",
    "    parser.add_argument('--content_layers', default='relu4_2', type=str,\n",
    "                        help='Layer to attach content loss.')\n",
    "    parser.add_argument('--style_layers', default='relu3_2, relu4_2', type=str,\n",
    "                        help='Layer to attach content loss.')\n",
    "\n",
    "    parser.add_argument('--use_segmentation_patch', default=True, type=bool,\n",
    "                        help='whether or not to inject noise into the network')\n",
    "\n",
    "    parser.add_argument('--input_texture_patch', default='dtd_texture', type=str,\n",
    "                        choices=['original_image', 'dtd_texture'],\n",
    "                        help='whether or not to inject noise into the network')\n",
    "    \n",
    "    parser.add_argument('--loss_texture', default='dtd_texture', type=str,\n",
    "                        choices=['original_image', 'dtd_texture'],\n",
    "                        help='where is the texture loss come from')\n",
    "    \n",
    "    parser.add_argument('--local_texture_size', default=50, type=int,\n",
    "                        help='use local texture loss instead of global, set -1 to use global')\n",
    "    \n",
    "    parser.add_argument('--texture_discrminator_loss', default=True, type=bool,\n",
    "                        help='adding discrminator for texture')\n",
    "    \n",
    "    return parser.parse_args(argv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Val Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "command = '--display_port 7775 --load 0 --load_D -1 --load_epoch 129 --gpu 2 --model scribbler --feature_weight 10 --pixel_weight_ab 1e3 --local_pixel_weight_l 1e3 --style_weight 1 --discriminator_weight 1e2 --discriminator_local_weight 0 --learning_rate 1e-2 --learning_rate_D 1e-4 --load_dir /home/wendy/scribbler_models_handbags_DTD_0 --save_dir /home/wendy/scribbler_models_handbags_DTD_0 --data_path /home/wxian3/training_handbags_pretrain/ --batch_size 45 --save_every 500 --num_epoch 100000 --input_texture_patch dtd_texture --loss_texture dtd_texture --local_texture_size 50 --num_input_texture_patch 1 --num_local_texture_patch 2 --patch_size_min 30 --patch_size_min 50'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "args = parse_arguments(command.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from main import get_transforms\n",
    "from models import save_network, load_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transform = get_transforms(args)\n",
    "val = imfol.make_dataset(args.data_path, 'val')\n",
    "valDset = ImageFolder('val', args.data_path, transform)\n",
    "val_display_size = args.batch_size\n",
    "indices = torch.randperm(len(valDset))\n",
    "val_display_sampler = SequentialSampler(indices[:val_display_size])\n",
    "valLoader = DataLoader(dataset=valDset, batch_size=val_display_size,sampler=val_display_sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from main import get_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded G from epoch: 129 itr: 0\n",
      "Loaded D from epoch: 129 itr: 0\n",
      "While copying the parameter named model.0.weight, whose dimensions in the model are torch.Size([32, 2, 3, 3]) and whose dimensions in the checkpoint are torch.Size([32, 2, 9, 9]), ...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "inconsistent tensor size, expected tensor [32 x 2 x 3 x 3] and src [32 x 2 x 9 x 9] to have the same number of elements, but got 576 and 5184 elements respectively at /opt/conda/conda-bld/pytorch_1503966894950/work/torch/lib/TH/generic/THTensorCopy.c:86",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-c7a82b030390>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnetG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetD_local\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/wendy/texturegan/main.py\u001b[0m in \u001b[0;36mget_models\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mload_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'D'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mload_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetD_local\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'D_local'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnetG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetD_local\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wendy/texturegan/models/__init__.pyc\u001b[0m in \u001b[0;36mload_network\u001b[0;34m(model, network_label, epoch, iteration, args)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m\"state_dict\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_state\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"state_dict\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict)\u001b[0m\n\u001b[1;32m    358\u001b[0m                 \u001b[0mparam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m                 \u001b[0mown_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 print('While copying the parameter named {}, whose dimensions in the model are'\n",
      "\u001b[0;31mRuntimeError\u001b[0m: inconsistent tensor size, expected tensor [32 x 2 x 3 x 3] and src [32 x 2 x 9 x 9] to have the same number of elements, but got 576 and 5184 elements respectively at /opt/conda/conda-bld/pytorch_1503966894950/work/torch/lib/TH/generic/THTensorCopy.c:86"
     ]
    }
   ],
   "source": [
    "netG, netD, netD_local = get_models(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from train import gen_input, rand_between, gen_input_rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_input_exact(img, skg, seg, xcenter=64, ycenter=64, crop_size=60, num_patch=1):\n",
    "    #generate input skg with random patch from img\n",
    "    #input img,skg [bsx3xwxh], xcenter,ycenter, size \n",
    "    #output bsx5xwxh\n",
    "    MAX_COUNT = 10000\n",
    "    bs,c,w,h = img.size()\n",
    "    results = torch.Tensor(bs,5,w,h)\n",
    "    texture_info = []\n",
    "\n",
    "    #text_info.append([xcenter,ycenter,crop_size])    \n",
    "    seg = seg/torch.max(seg)\n",
    "    counter = 0\n",
    "    for i in range(bs):\n",
    "        counter=0\n",
    "        ini_texture = torch.ones(img[0].size())*(1)\n",
    "        ini_mask =  torch.ones((1,w,h))*(-1)\n",
    "        temp_info = []\n",
    "        for j in range(num_patch):\n",
    "            xstart = max(int(xcenter-crop_size/2), 0)\n",
    "            ystart = max(int(ycenter-crop_size/2), 0)\n",
    "            xend = min(int(xcenter + crop_size/2), w)\n",
    "            yend = min(int(ycenter + crop_size/2), h)\n",
    "            patch = seg[i, xstart:xend, ystart:yend]\n",
    "            sizem = torch.ones(patch.size())            \n",
    "\n",
    "            temp_info.append([xcenter, ycenter, crop_size])\n",
    "            res = gen_input(img[i], skg[i], ini_texture, ini_mask, xcenter, ycenter, crop_size)\n",
    "          \n",
    "            ini_texture = res[1:4,:,:]\n",
    "            \n",
    "        texture_info.append(temp_info)\n",
    "        results[i,:,:,:] = res\n",
    "    return results, texture_info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = valLoader.__iter__().__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-ad9d5aa69fad>\u001b[0m in \u001b[0;36mmove_patch\u001b[0;34m(x, xcenter, ycenter, size)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0minteract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxcenter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mycenter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmove_patch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxcenter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mycenter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meroded_seg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtxt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m  \u001b[0;31m# LAB with negeative value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m# this is in LAB value 0/100, -128/128 etc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "@interact(x=[0,25],xcenter=[0,128],ycenter=[0,128],size=[10,100])\n",
    "def move_patch(x,xcenter,ycenter,size):\n",
    "    img, skg, seg, eroded_seg, txt = data  # LAB with negeative value\n",
    "    with torch.cuda.device(args.gpu):\n",
    "        # this is in LAB value 0/100, -128/128 etc\n",
    "        img = custom_transforms.normalize_lab(img)\n",
    "        skg = custom_transforms.normalize_lab(skg)\n",
    "        txt = custom_transforms.normalize_lab(txt)\n",
    "        seg = custom_transforms.normalize_seg(seg)\n",
    "        eroded_seg = custom_transforms.normalize_seg(eroded_seg)\n",
    "        \n",
    "        bs, w, h = seg.size()\n",
    "\n",
    "        seg = seg.view(bs, 1, w, h)\n",
    "        seg = torch.cat((seg, seg, seg), 1)\n",
    "        \n",
    "        eroded_seg = eroded_seg.view(bs, 1, w, h)\n",
    "        eroded_seg = torch.cat((eroded_seg, eroded_seg, eroded_seg), 1)\n",
    "\n",
    "        temp = torch.ones(seg.size()) * (1 - seg).float()\n",
    "        temp[:, 1, :, :] = 0  # torch.ones(seg[:,1,:,:].size())*(1-seg[:,1,:,:]).float()\n",
    "        temp[:, 2, :, :] = 0  # torch.ones(seg[:,2,:,:].size())*(1-seg[:,2,:,:]).float()\n",
    "\n",
    "        txt = txt.float() * seg.float() + temp\n",
    "        # seg=custom_transforms.normalize_lab(seg)\n",
    "        # norm to 0-1 minus mean\n",
    "        if not args.use_segmentation_patch:\n",
    "            seg.fill_(1)\n",
    "            eroded_seg.fill_(1)\n",
    "        if args.input_texture_patch == 'original_image':\n",
    "            inp, texture_loc = gen_input_rand(img, skg, eroded_seg[:, 0, :, :] * 100,\n",
    "                                              args.patch_size_min, args.patch_size_max,\n",
    "                                              args.num_input_texture_patch)\n",
    "        elif args.input_texture_patch == 'dtd_texture':\n",
    "            inp, texture_loc = gen_input_rand(txt, skg, eroded_seg[:, 0, :, :] * 100,\n",
    "                                              args.patch_size_min, args.patch_size_max,\n",
    "                                              args.num_input_texture_patch)\n",
    "\n",
    "        img = img.cuda()\n",
    "        skg = skg.cuda()\n",
    "        seg = seg.cuda()\n",
    "        txt = txt.cuda()\n",
    "        inp = inp.cuda()\n",
    "        \n",
    "        input_stack = torch.FloatTensor().cuda()\n",
    "        input_stack.resize_as_(inp.float()).copy_(inp)\n",
    "        #target_img.resize_as_(img.float()).copy_(img)\n",
    "        #segment.resize_as_(seg.float()).copy_(seg)\n",
    "\n",
    "        inputv = Variable(input_stack)\n",
    "        #targetv = Variable(target_img)\n",
    "        \n",
    "        netG.cuda()\n",
    "        outputG = netG(inputv)\n",
    "\n",
    "    if args.color_space == 'lab':\n",
    "        out_img = vis_image(custom_transforms.denormalize_lab(outputG.data.double().cpu()),\n",
    "                            args.color_space)\n",
    "        if args.input_texture_patch == 'original_image':\n",
    "            inp_img = vis_patch(custom_transforms.denormalize_lab(img.cpu()),\n",
    "                                custom_transforms.denormalize_lab(skg.cpu()),\n",
    "                                texture_loc,\n",
    "                                args.color_space)\n",
    "        elif args.input_texture_patch == 'dtd_texture':\n",
    "            inp_img = vis_patch(custom_transforms.denormalize_lab(txt.cpu()),\n",
    "                                custom_transforms.denormalize_lab(skg.cpu()),\n",
    "                                texture_loc,\n",
    "                                args.color_space)\n",
    "        tar_img = vis_image(custom_transforms.denormalize_lab(img.cpu()),\n",
    "                            args.color_space)\n",
    "    elif args.color_space == 'rgb':\n",
    "\n",
    "        out_img = vis_image(custom_transforms.denormalize_rgb(outputG.data.double().cpu()),\n",
    "                            args.color_space)\n",
    "        inp_img = vis_patch(custom_transforms.denormalize_rgb(img.cpu()),\n",
    "                            custom_transforms.denormalize_rgb(skg.cpu()),\n",
    "                            texture_loc,\n",
    "                            args.color_space)\n",
    "        tar_img = vis_image(custom_transforms.denormalize_rgb(img.cpu()),\n",
    "                            args.color_space)\n",
    "        \n",
    "    plt.figure(1)    \n",
    "    plt.imshow(np.transpose(inp_img[x],(1, 2, 0)))\n",
    "    \n",
    "    plt.figure(2)                    \n",
    "    plt.imshow(np.transpose(out_img[x],(1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
