{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from __future__ import print_function\n",
    "#from __future__ import absolute_import\n",
    "#from __future__ import division\n",
    "\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision.datasets as dataset\n",
    "import visdom\n",
    "import sys,argparse,os\n",
    "\n",
    "from models import scribbler, discriminator\n",
    "import torch.optim as optim\n",
    "\n",
    "from skimage import color\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import visualize\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from IPython.display import display\n",
    "import torchvision.models as models\n",
    "from dataloader import imfol\n",
    "\n",
    "from utils.visualize import vis_patch, vis_image\n",
    "        \n",
    "from torch.utils.data import DataLoader\n",
    "from dataloader.imfol import ImageFolder, make_dataset\n",
    "from utils import transforms as custom_trans\n",
    "import torchvision.transforms as tforms\n",
    "import utils.transforms as utforms\n",
    "\n",
    "from networks import define_G, weights_init\n",
    "from models import scribbler \n",
    "import visdom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clamp_image(img):\n",
    "    img[:,0,:,:].clamp_(0,1)\n",
    "    img[:,1,:,:].clamp_(-1.5,1.5)\n",
    "    img[:,2,:,:].clamp_(-1.5,1.5)\n",
    "    return img    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_arguments(argv):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "###############added options#######################################\n",
    "    parser.add_argument('-learning_rate', default=1e-4, type=float,\n",
    "                    help='Learning rate for the generator')\n",
    "    parser.add_argument('-learning_rate_D',  default=1e-4,type=float,\n",
    "                    help='Learning rate for the discriminator')    \n",
    "    \n",
    "    parser.add_argument('-gan', default='dcgan',type=str,choices=['dcgan', 'lsgan'],\n",
    "                    help='dcgan|lsgan') #todo wgan/improved wgan    \n",
    "    \n",
    "    parser.add_argument('-model', default='pix2pix',type=str,choices=['scribbler', 'pix2pix'],\n",
    "                   help='scribbler|pix2pix')\n",
    "    \n",
    "    parser.add_argument('-num_epoch',  default=1,type=int,\n",
    "                    help='texture|scribbler')   \n",
    "    \n",
    "    parser.add_argument('-visualize_every',  default=10,type=int,\n",
    "                    help='no. iteration to visualize the results')      \n",
    "\n",
    "    #all the weights ratio, might wanna make them sum to one\n",
    "    parser.add_argument('-feature_weight', default=10,type=float,\n",
    "                       help='weight ratio for feature loss')\n",
    "    parser.add_argument('-pixel_weight_l', default=1,type=float,\n",
    "                       help='weight ratio for pixel loss for l channel')\n",
    "    parser.add_argument('-pixel_weight_ab', default=10,type=float,\n",
    "                   help='weight ratio for pixel loss for ab channel')\n",
    "    parser.add_argument('-tv_weight', default=1,type=float,\n",
    "                   help='weight ratio for total variation loss')\n",
    "    parser.add_argument('-discriminator_weight', default=0,type=float,\n",
    "                   help='weight ratio for the discriminator loss')\n",
    "\n",
    "    parser.add_argument('-gpu', default=1,type=int,\n",
    "                   help='id of gpu to use') #TODO support cpu\n",
    "\n",
    "    parser.add_argument('-display_port', default=7779,type=int,\n",
    "               help='port for displaying on visdom (need to match with visdom currently open port)')\n",
    "\n",
    "    parser.add_argument('-data_path', default='/home/psangkloy3/training_handbags_pretrain/',type=str,\n",
    "                   help='path to the data directory, expect train_skg, train_img, val_skg, val_img')\n",
    "\n",
    "    parser.add_argument('-save_dir', default='/home/psangkloy3/texturegan/save_dir',type=str,\n",
    "                   help='path to save the model')\n",
    "    parser.add_argument('-save_every',  default=1,type=int,\n",
    "                    help='no. iteration to save the models')\n",
    "    \n",
    "############################################################################\n",
    "############################################################################\n",
    "############TODO: TO ADD#################################################################\n",
    "    parser.add_argument('-content_layers',  default='relu2_2',type=str,\n",
    "                    help='Layer to attach content loss.')\n",
    "    \n",
    "    parser.add_argument('-batch_size', default=1) #fixed batch size 1\n",
    "    \n",
    "    parser.add_argument('-image_size',default=128,type=int,\n",
    "                    help='Training images size, after cropping')        \n",
    "    parser.add_argument('-resize_max',  default=256,type=int,\n",
    "                    help='max resize size')        \n",
    "    parser.add_argument('-resize_min',  default=128,type=int,\n",
    "                    help='min resize size')   \n",
    "    \n",
    "\n",
    "    \n",
    "    parser.add_argument('-mode',  default='texture',type=str,choices=['texture','scribbler'],\n",
    "                    help='texture|scribbler') \n",
    "    \n",
    "   \n",
    "    parser.add_argument('-crop',  default='random',type=str,choices=['random','center'],\n",
    "                    help='random|center')\n",
    "    \n",
    "    parser.add_argument('-contrast',  default=True,type=bool,\n",
    "                    help='randomly adjusting contrast on sketch')\n",
    "    \n",
    "    parser.add_argument('-occlude', default=False,type=bool,\n",
    "                       help='randomly occlude part of the sketch')\n",
    "    \n",
    "    \n",
    "    parser.add_argument('-checkpoints_path', default='data/',type=str,\n",
    "                   help='output directory for results and models')\n",
    "    \n",
    "\n",
    "    \n",
    "    parser.add_argument('-noise_gen', default=False,type=bool,\n",
    "                   help='whether or not to inject noise into the network')\n",
    "    \n",
    "    parser.add_argument('-load', default=1,type=int,\n",
    "                   help='load generator and discrminator from iteration n')\n",
    "    parser.add_argument('-load_D', default=1,type=float,\n",
    "                   help='load discriminator from iteration n, priority over load')\n",
    "    \n",
    "    parser.add_argument('-absolute_load', default='',type=str,\n",
    "                   help='load saved generator model from absolute location')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "##################################################################################################################################    \n",
    "    \n",
    "    return parser.parse_args(argv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "command = '-content_layers relu2_2 -feature_weight 100 -pixel_weight_ab 1 -tv_weight 0.0001 -model scribbler_custom -data_path /home/psangkloy3/training_handbag_pretrain/ -gpu 1 -display_port 7779 -image_size 128 -save_every 5000 -visualize_every 10 -discriminator_weight 0 -learning_rate 1e-3 -learning_rate_D 1e-6 -batch_size 6 -contrast True -resize_max 256 -resize_min 128 -gan lsgan -load 100000'\n",
    "command = ''\n",
    "args = parse_arguments(command.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4.39800643921\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with torch.cuda.device(args.gpu):\n",
    "    \n",
    "    vis=visdom.Visdom(port=args.display_port)\n",
    "    \n",
    "    Loss_g_graph=[]\n",
    "    Loss_gd_graph=[]\n",
    "    Loss_gf_graph = []\n",
    "    Loss_gp_graph = []\n",
    "    Loss_d_graph=[]\n",
    "\n",
    "    ts=tforms.Compose([custom_trans.toLAB(), custom_trans.toTensor()])\n",
    "    rgbify = custom_trans.toRGB()\n",
    "    dset = ImageFolder(args.data_path,ts)\n",
    "    dataloader=DataLoader(dataset=dset, batch_size=2, shuffle=True)\n",
    "    \n",
    "    sigmoid_flag = 1\n",
    "    if args.gan =='lsgan':\n",
    "        sigmoid_flag = 0 \n",
    "        \n",
    "    if args.model=='scribbler':\n",
    "        netG=scribbler.Scribbler(3,3,32)\n",
    "    elif args.model=='pix2pix':\n",
    "        netG=define_G(3,3,32)\n",
    "    else:\n",
    "        print(argv.model+ ' not support. Using pix2pix model')\n",
    "        netG=define_G(3,3,32)\n",
    "    netD=discriminator.Discriminator(3,32,sigmoid_flag)  \n",
    "    feat_model=models.vgg19(pretrained=True)\n",
    "\n",
    "    netG.apply(weights_init)\n",
    "    netD.apply(weights_init)    \n",
    "\n",
    "    \n",
    "    if args.gan =='lsgan':\n",
    "        criterion_gan = nn.MSELoss()\n",
    "    elif args.gan =='dcgan':\n",
    "        criterion_gan = nn.BCELoss()\n",
    "        \n",
    "    criterion_l1 = nn.L1Loss()\n",
    "    \n",
    "    criterion_feat = nn.L1Loss()\n",
    "    \n",
    "    \n",
    "\n",
    "    input_skg = torch.FloatTensor(2, 3, 256, 256)\n",
    "    output_img = torch.FloatTensor(2, 3, 256, 256)\n",
    "    segment = torch.FloatTensor(2, 3, 256, 256)\n",
    "    label = torch.FloatTensor(2)\n",
    "    real_label = 1\n",
    "    fake_label = 0\n",
    "\n",
    "    optimizerD = optim.Adam(netD.parameters(), lr=args.learning_rate_D, betas=(0.5, 0.999))\n",
    "    optimizerG = optim.Adam(netG.parameters(), lr=args.learning_rate, betas=(0.5, 0.999))\n",
    "\n",
    "    netG.cuda()\n",
    "    netD.cuda()\n",
    "    feat_model.cuda()\n",
    "    criterion_gan.cuda()\n",
    "    criterion_l1.cuda()\n",
    "    criterion_feat.cuda()\n",
    "    input_skg, output_img, segment, label = input_skg.cuda(), output_img.cuda(),segment.cuda(), label.cuda()\n",
    "\n",
    "\n",
    "    for epoch in range(args.num_epoch):\n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "            ############################\n",
    "            # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "            ###########################\n",
    "            # train with real\n",
    "            netD.zero_grad()\n",
    "            img, skg,seg = data\n",
    "            img=utforms.normalize_lab(img)\n",
    "            skg=utforms.normalize_lab(skg)\n",
    "            \n",
    "            img=img.cuda()\n",
    "            skg=skg.cuda()\n",
    "            seg=seg.cuda()\n",
    "            \n",
    "            input_skg.resize_as_(skg.float()).copy_(skg)\n",
    "            output_img.resize_as_(img.float()).copy_(img)\n",
    "            segment.resize_as_(seg.float()).copy_(seg)\n",
    "            \n",
    "            inputv = Variable(input_skg)\n",
    "            outputv = Variable(output_img)\n",
    "            labelv = Variable(label)\n",
    "            #print labelv.data.size()\n",
    "\n",
    "            output = netD(inputv)\n",
    "            \n",
    "            label.resize_(output.data.size())\n",
    "            labelv = Variable(label.fill_(real_label))\n",
    "            errD_real = criterion_gan(output, labelv)\n",
    "            errD_real.backward()\n",
    "            D_x = output.data.mean()\n",
    "\n",
    "            # train with fake\n",
    "            #noise.resize_(batch_size, nz, 1, 1).normal_(0, 1)\n",
    "            #noisev = Variable(noise)\n",
    "            fake = netG(inputv)\n",
    "            output = netD(fake.detach())\n",
    "            label.resize_(output.data.size())\n",
    "            labelv = Variable(label.fill_(fake_label))\n",
    "\n",
    "            errD_fake = criterion_gan(output, labelv)\n",
    "            errD_fake.backward()\n",
    "            D_G_z1 = output.data.mean()\n",
    "            errD = errD_real + errD_fake\n",
    "            Loss_d_graph.append(errD.data[0])\n",
    "            optimizerD.step()\n",
    "\n",
    "            ############################\n",
    "            # (2) Update G network: maximize log(D(G(z)))\n",
    "            ###########################\n",
    "            netG.zero_grad()\n",
    "            labelv = Variable(label.fill_(real_label))  # fake labels are real for generator cost\n",
    "            output = netD(fake)\n",
    "\n",
    "            multer=torch.ones(inputv.data.size())\n",
    "            multer[:,1:,:,:]=args.pixel_weight_ab*multer[:,1:,:,:]\n",
    "            multer=multer.cuda()\n",
    "            multer=Variable(multer)\n",
    "            \n",
    "            multer_=torch.ones(inputv.data.size())\n",
    "            multer_[:,0,:,:]=args.pixel_weight_l*multer_[:,0,:,:]\n",
    "            multer_=multer_.cuda()\n",
    "            multer_=Variable(multer_)\n",
    "            \n",
    "            #print outputv.size(), multer.size()\n",
    "            woutputv = outputv*multer*multer_\n",
    "            err_pixel = criterion_l1(fake*multer, woutputv)\n",
    "            #####################################\n",
    "            D_G_z2 = output.data.mean()\n",
    "            \n",
    "            label.resize_(output.data.size())\n",
    "            labelv = Variable(label.fill_(real_label))\n",
    "\n",
    "            err_gan = args.discriminator_weight*criterion_gan(output, labelv)\n",
    "            \n",
    "            ####################################\n",
    "            #TODO normalize and minus mean?\n",
    "            L,A,B=torch.chunk(fake,3,dim=1)\n",
    "            LLL=torch.cat((L,L,L),1)\n",
    "            out_feat=feat_model.features(LLL)\n",
    "            \n",
    "            #print(LLL.size())\n",
    "            #break\n",
    "            gt_feat = feat_model.features(outputv)\n",
    "            \n",
    "            gt_feat = gt_feat.detach() #don't require grad for this\n",
    "            \n",
    "            err_feat = args.feature_weight*criterion_feat(out_feat,gt_feat)\n",
    "            #err_feat.backward()\n",
    "            \n",
    "            err_G = err_pixel + err_gan + err_feat\n",
    "            err_G.backward()\n",
    "            \n",
    "            optimizerG.step()\n",
    "            Loss_g_graph.append(err_G.data[0])\n",
    "            Loss_gp_graph.append(err_pixel.data[0])\n",
    "            Loss_gd_graph.append(err_gan.data[0])\n",
    "            Loss_gf_graph.append(err_feat.data[0])\n",
    "            #plt.imshow(vis_image(inputv.data.double().cpu()))\n",
    "\n",
    "            print i, err_G.data[0]\n",
    "            \n",
    "            if(i%args.save_every==0):\n",
    "                save_network(netG,'G',i,args.gpu,args.save_dir)\n",
    "                save_network(netD,'D',i,args.gpu,args.save_dir)\n",
    "                \n",
    "                \n",
    "            #TODO test on test set\n",
    "            if(i%args.visualize_every==0):\n",
    "                test_img=clamp_image(fake.data.double().cpu())\n",
    "                test_img=utforms.denormalize_lab(test_img)\n",
    "                test_img=vis_image(test_img)\n",
    "                test_img=(test_img*255).astype('uint8')\n",
    "                test_img=np.transpose(test_img,(2,0,1))\n",
    "\n",
    "                inp_img=vis_patch(utforms.denormalize_lab(img.cpu()),utforms.denormalize_lab(skg.cpu()))\n",
    "                inp_img=(inp_img*255).astype('uint8')\n",
    "                inp_img=np.transpose(inp_img,(2,0,1))\n",
    "                \n",
    "                target_img=vis_image(utforms.denormalize_lab(img.cpu()))\n",
    "                target_img=(target_img*255).astype('uint8')\n",
    "                target_img=np.transpose(target_img,(2,0,1))\n",
    "                \n",
    "                segment_img=vis_image((seg.cpu()))\n",
    "                segment_img=(segment_img*255).astype('uint8')\n",
    "                segment_img=np.transpose(segment_img,(2,0,1))\n",
    "                \n",
    "                vis.image(test_img,win='output',opts=dict(title='output'))\n",
    "                vis.image(inp_img,win='input',opts=dict(title='input'))  \n",
    "                vis.image(target_img,win='target',opts=dict(title='target'))\n",
    "                vis.image(segment_img,win='segment',opts=dict(title='segment'))\n",
    "                vis.line(np.array(Loss_g_graph),win='g',opts=dict(title='G Total Loss'))\n",
    "                vis.line(np.array(Loss_gd_graph),win='gd',opts=dict(title='G-Discriminator Loss'))\n",
    "                vis.line(np.array(Loss_gf_graph),win='gf',opts=dict(title='G-Feature Loss'))\n",
    "                vis.line(np.array(Loss_gp_graph),win='gp',opts=dict(title='G-Pixel Loss'))\n",
    "                vis.line(np.array(Loss_d_graph),win='d',opts=dict(title='D Loss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def save_network(model, network_label, epoch_label, gpu_id, save_dir):\n",
    "    save_filename = '%s_net_%s.pth' % (epoch_label, network_label)\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    save_path = os.path.join(save_dir, save_filename)\n",
    "    torch.save(model.cpu().state_dict(), save_path)\n",
    "    model.cuda(device_id=gpu_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "tensors are on different GPUs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-b3d643d97e14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlab_var\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mLLL\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeat_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLLL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/root/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda2/lib/python2.7/site-packages/torch/nn/modules/container.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda2/lib/python2.7/site-packages/torch/nn/modules/conv.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 254\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda2/lib/python2.7/site-packages/torch/nn/functional.pyc\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, weight, bias, stride, padding, dilation, groups)\u001b[0m\n\u001b[1;32m     50\u001b[0m     f = ConvNd(_pair(stride), _pair(padding), _pair(dilation), False,\n\u001b[1;32m     51\u001b[0m                _pair(0), groups, torch.backends.cudnn.benchmark, torch.backends.cudnn.enabled)\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: tensors are on different GPUs"
     ]
    }
   ],
   "source": [
    "#TODO normalize and minus mean\n",
    "feat_model=models.vgg19(pretrained=True)\n",
    "lab_var = fake\n",
    "L,A,B=torch.chunk(lab_var,3,dim=1)\n",
    "LLL=torch.cat((L,L,L),1)\n",
    "out=feat_model.features(LLL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
