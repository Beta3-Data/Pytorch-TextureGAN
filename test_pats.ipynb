{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from __future__ import print_function\n",
    "#from __future__ import absolute_import\n",
    "#from __future__ import division\n",
    "\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision.datasets as dataset\n",
    "import visdom\n",
    "import sys,argparse\n",
    "\n",
    "from models import scribbler, discriminator\n",
    "import torch.optim as optim\n",
    "\n",
    "from skimage import color\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import visualize\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from IPython.display import display\n",
    "import torchvision.models as models\n",
    "from dataloader import imfol\n",
    "\n",
    "from utils.visualize import vis_patch, vis_image\n",
    "        \n",
    "from torch.utils.data import DataLoader\n",
    "from dataloader.imfol import ImageFolder, make_dataset\n",
    "from utils import transforms as custom_trans\n",
    "import torchvision.transforms as tforms\n",
    "import utils.transforms as utforms\n",
    "\n",
    "from networks import define_G, weights_init\n",
    "\n",
    "import visdom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clamp_image(img):\n",
    "    img[:,0,:,:].clamp_(0,1)\n",
    "    img[:,1,:,:].clamp_(-1.5,1.5)\n",
    "    img[:,2,:,:].clamp_(-1.5,1.5)\n",
    "    return img    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_arguments(argv):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "###############added options#######################################\n",
    "    parser.add_argument('-learning_rate', default=1e-4, type=float,\n",
    "                    help='Learning rate for the generator')\n",
    "    parser.add_argument('-learning_rate_D',  default=1e-4,type=float,\n",
    "                    help='Learning rate for the discriminator')    \n",
    "    \n",
    "    parser.add_argument('-gan', default='dcgan',type=str,choices=['dcgan', 'lsgan'],\n",
    "                    help='dcgan|lsgan') #todo wgan/improved wgan    \n",
    "    \n",
    "    parser.add_argument('-num_epoch',  default=1,type=int,\n",
    "                    help='texture|scribbler')   \n",
    "    \n",
    "    parser.add_argument('-visualize_every',  default=10,type=int,\n",
    "                    help='no. iteration to visualize the results')      \n",
    "\n",
    "    #all the weights ratio, might wanna make them sum to one\n",
    "    parser.add_argument('-feature_weight', default=1,type=float,\n",
    "                       help='weight ratio for feature loss')\n",
    "    parser.add_argument('-pixel_weight_l', default=0,type=float,\n",
    "                       help='weight ratio for pixel loss for l channel')\n",
    "    parser.add_argument('-pixel_weight_ab', default=0,type=float,\n",
    "                   help='weight ratio for pixel loss for ab channel')\n",
    "    parser.add_argument('-tv_weight', default=1,type=float,\n",
    "                   help='weight ratio for total variation loss')\n",
    "    parser.add_argument('-discriminator_weight', default=0,type=float,\n",
    "                   help='weight ratio for the discriminator loss')\n",
    "\n",
    "    parser.add_argument('-gpu', default=1,type=int,\n",
    "                   help='id of gpu to use, -1 for cpu')\n",
    "\n",
    "    parser.add_argument('-display_port', default=7779,type=int,\n",
    "               help='port for displaying on visdom (need to match with visdom currently open port)')\n",
    "\n",
    "    parser.add_argument('-data_path', default='/home/psangkloy3/training_handbags_pretrain/',type=str,\n",
    "                   help='path to the data directory, expect train_skg, train_img, val_skg, val_img')\n",
    "\n",
    "\n",
    "############################################################################\n",
    "############################################################################\n",
    "############TODO: TO ADD#################################################################\n",
    "    parser.add_argument('-content_layers',  default='relu2_2',type=str,\n",
    "                    help='Layer to attach content loss.')\n",
    "    \n",
    "    parser.add_argument('-batch_size', default=1) #fixed batch size 1\n",
    "    \n",
    "    parser.add_argument('-image_size',default=128,type=int,\n",
    "                    help='Training images size, after cropping')        \n",
    "    parser.add_argument('-resize_max',  default=256,type=int,\n",
    "                    help='max resize size')        \n",
    "    parser.add_argument('-resize_min',  default=128,type=int,\n",
    "                    help='min resize size')   \n",
    "    \n",
    "\n",
    "    \n",
    "    parser.add_argument('-mode',  default='texture',type=str,choices=['texture','scribbler'],\n",
    "                    help='texture|scribbler') \n",
    "    \n",
    "    parser.add_argument('-save_every',  default=50000,type=int,\n",
    "                    help='no. iteration to save the models')\n",
    "    \n",
    "    parser.add_argument('-crop',  default='random',type=str,choices=['random','center'],\n",
    "                    help='random|center')\n",
    "    \n",
    "    parser.add_argument('-contrast',  default=True,type=bool,\n",
    "                    help='randomly adjusting contrast on sketch')\n",
    "    \n",
    "    parser.add_argument('-occlude', default=False,type=bool,\n",
    "                       help='randomly occlude part of the sketch')\n",
    "    \n",
    "    \n",
    "    parser.add_argument('-checkpoints_path', default='data/',type=str,\n",
    "                   help='output directory for results and models')\n",
    "    \n",
    "    parser.add_argument('-model', default='scribbler_custom',type=str,\n",
    "                   help='generator architecture')\n",
    "    \n",
    "    parser.add_argument('-noise_gen', default=False,type=bool,\n",
    "                   help='whether or not to inject noise into the network')\n",
    "    \n",
    "    parser.add_argument('-load', default=1,type=int,\n",
    "                   help='load generator and discrminator from iteration n')\n",
    "    parser.add_argument('-load_D', default=1,type=float,\n",
    "                   help='load discriminator from iteration n, priority over load')\n",
    "    \n",
    "    parser.add_argument('-absolute_load', default='',type=str,\n",
    "                   help='load saved generator model from absolute location')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "##################################################################################################################################    \n",
    "    \n",
    "    return parser.parse_args(argv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "command = '-content_layers relu2_2 -feature_weight 100 -pixel_weight_ab 1 -tv_weight 0.0001 -model scribbler_custom -data_path /home/psangkloy3/training_handbag_pretrain/ -gpu 1 -display_port 7779 -image_size 128 -save_every 5000 -visualize_every 10 -discriminator_weight 0 -learning_rate 1e-3 -learning_rate_D 1e-6 -batch_size 6 -contrast True -resize_max 256 -resize_min 128 -gan lsgan -load 100000'\n",
    "command = ''\n",
    "argv = parse_arguments(command.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with torch.cuda.device(argv.gpu):\n",
    "    \n",
    "    vis=visdom.Visdom(port=argv.display_port)\n",
    "    \n",
    "    Loss_g_graph=[]\n",
    "    Loss_gd_graph=[]\n",
    "    Loss_gf_graph = []\n",
    "    Loss_gp_graph = []\n",
    "    Loss_d_graph=[]\n",
    "\n",
    "    ts=tforms.Compose([custom_trans.toLAB(), custom_trans.toTensor()])\n",
    "    rgbify = custom_trans.toRGB()\n",
    "    dset = ImageFolder(argv.data_path,ts)\n",
    "    dataloader=DataLoader(dataset=dset, batch_size=2, shuffle=True)\n",
    "    \n",
    "    sigmoid_flag = 1\n",
    "    if argv.gan =='lsgan':\n",
    "        sigmoid_flag = 0 \n",
    "        \n",
    "\n",
    "    netG=define_G(3,3,32)\n",
    "    netD=discriminator.Discriminator(3,32,sigmoid_flag)  \n",
    "    feat_model=models.vgg19(pretrained=True)\n",
    "\n",
    "    netG.apply(weights_init)\n",
    "    netD.apply(weights_init)    \n",
    "\n",
    "    \n",
    "    if argv.gan =='lsgan':\n",
    "        criterion_gan = nn.MSELoss()\n",
    "    elif argv.gan =='dcgan':\n",
    "        criterion_gan = nn.BCELoss()\n",
    "        \n",
    "    criterion_l1 = nn.L1Loss()\n",
    "    \n",
    "    criterion_feat = nn.L1Loss()\n",
    "    \n",
    "    \n",
    "\n",
    "    input_skg = torch.FloatTensor(2, 3, 256, 256)\n",
    "    output_img = torch.FloatTensor(2, 3, 256, 256)\n",
    "    label = torch.FloatTensor(2)\n",
    "    real_label = 1\n",
    "    fake_label = 0\n",
    "\n",
    "    optimizerD = optim.Adam(netD.parameters(), lr=argv.learning_rate_D, betas=(0.5, 0.999))\n",
    "    optimizerG = optim.Adam(netG.parameters(), lr=argv.learning_rate, betas=(0.5, 0.999))\n",
    "\n",
    "    netG.cuda()\n",
    "    netD.cuda()\n",
    "    feat_model.cuda()\n",
    "    criterion_gan.cuda()\n",
    "    criterion_l1.cuda()\n",
    "    criterion_feat.cuda()\n",
    "    input_skg, output_img, label = input_skg.cuda(), output_img.cuda(), label.cuda()\n",
    "\n",
    "\n",
    "    for epoch in range(argv.num_epoch):\n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "            ############################\n",
    "            # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "            ###########################\n",
    "            # train with real\n",
    "            netD.zero_grad()\n",
    "            img, skg = data\n",
    "            img=utforms.normalize_lab(img)\n",
    "            skg=utforms.normalize_lab(skg)\n",
    "            img=img.cuda()\n",
    "            skg=skg.cuda()\n",
    "            input_skg.resize_as_(skg.float()).copy_(skg)\n",
    "            output_img.resize_as_(img.float()).copy_(img)\n",
    "            inputv = Variable(input_skg)\n",
    "            outputv = Variable(output_img)\n",
    "            labelv = Variable(label)\n",
    "            #print labelv.data.size()\n",
    "\n",
    "            output = netD(inputv)\n",
    "            \n",
    "            label.resize_(output.data.size())\n",
    "            labelv = Variable(label.fill_(real_label))\n",
    "            errD_real = criterion_gan(output, labelv)\n",
    "            errD_real.backward()\n",
    "            D_x = output.data.mean()\n",
    "\n",
    "            # train with fake\n",
    "            #noise.resize_(batch_size, nz, 1, 1).normal_(0, 1)\n",
    "            #noisev = Variable(noise)\n",
    "            fake = netG(inputv)\n",
    "            output = netD(fake.detach())\n",
    "            label.resize_(output.data.size())\n",
    "            labelv = Variable(label.fill_(fake_label))\n",
    "\n",
    "            errD_fake = criterion_gan(output, labelv)\n",
    "            errD_fake.backward()\n",
    "            D_G_z1 = output.data.mean()\n",
    "            errD = errD_real + errD_fake\n",
    "            Loss_d_graph.append(errD.data[0])\n",
    "            optimizerD.step()\n",
    "\n",
    "            ############################\n",
    "            # (2) Update G network: maximize log(D(G(z)))\n",
    "            ###########################\n",
    "            netG.zero_grad()\n",
    "            labelv = Variable(label.fill_(real_label))  # fake labels are real for generator cost\n",
    "            output = netD(fake)\n",
    "\n",
    "            multer=torch.ones(inputv.data.size())\n",
    "            multer[:,1:,:,:]=argv.pixel_weight_ab*multer[:,1:,:,:]\n",
    "            multer=multer.cuda()\n",
    "            multer=Variable(multer)\n",
    "            \n",
    "            multer_=torch.ones(inputv.data.size())\n",
    "            multer_[:,0,:,:]=argv.pixel_weight_l*multer_[:,0,:,:]\n",
    "            multer_=multer_.cuda()\n",
    "            multer_=Variable(multer_)\n",
    "            \n",
    "            #print outputv.size(), multer.size()\n",
    "            woutputv = outputv*multer*multer_\n",
    "            err_pixel = criterion_l1(fake*multer, woutputv)\n",
    "            #####################################\n",
    "            D_G_z2 = output.data.mean()\n",
    "            \n",
    "            label.resize_(output.data.size())\n",
    "            labelv = Variable(label.fill_(real_label))\n",
    "\n",
    "            err_gan = argv.discriminator_weight*criterion_gan(output, labelv)\n",
    "            \n",
    "            ####################################\n",
    "            #TODO normalize and minus mean?\n",
    "            L,A,B=torch.chunk(fake,3,dim=1)\n",
    "            LLL=torch.cat((L,L,L),1)\n",
    "            out_feat=feat_model.features(LLL)\n",
    "            \n",
    "            #print(LLL.size())\n",
    "            #break\n",
    "            gt_feat = feat_model.features(outputv)\n",
    "            \n",
    "            gt_feat = gt_feat.detach() #don't require grad for this\n",
    "            \n",
    "            err_feat = argv.feature_weight*criterion_feat(out_feat,gt_feat)\n",
    "            #err_feat.backward()\n",
    "            \n",
    "            err_G = err_pixel + err_gan + err_feat\n",
    "            err_G.backward()\n",
    "            \n",
    "            optimizerG.step()\n",
    "            Loss_g_graph.append(err_G.data[0])\n",
    "            Loss_gp_graph.append(err_pixel.data[0])\n",
    "            Loss_gd_graph.append(err_gan.data[0])\n",
    "            Loss_gf_graph.append(err_feat.data[0])\n",
    "            #plt.imshow(vis_image(inputv.data.double().cpu()))\n",
    "\n",
    "            print i, err_G.data[0]\n",
    "            #TODO test on test set\n",
    "            if(i%argv.visualize_every==0):\n",
    "                test_img=clamp_image(fake.data.double().cpu())\n",
    "                test_img=utforms.denormalize_lab(test_img)\n",
    "                test_img=vis_image(test_img)\n",
    "                test_img=(test_img*255).astype('uint8')\n",
    "                test_img=np.transpose(test_img,(2,0,1))\n",
    "\n",
    "                inp_img=vis_patch(utforms.denormalize_lab(img.cpu()),utforms.denormalize_lab(skg.cpu()))\n",
    "                inp_img=(inp_img*255).astype('uint8')\n",
    "                inp_img=np.transpose(inp_img,(2,0,1))\n",
    "                \n",
    "                target_img=vis_image(utforms.denormalize_lab(img.cpu()))\n",
    "                target_img=(target_img*255).astype('uint8')\n",
    "                target_img=np.transpose(target_img,(2,0,1))\n",
    "                \n",
    "                vis.image(test_img,win='output',opts=dict(title='output'))\n",
    "                vis.image(inp_img,win='input',opts=dict(title='input'))  \n",
    "                vis.image(inp_img,win='input',opts=dict(title='input'))\n",
    "                vis.line(np.array(Loss_g_graph),win='g',opts=dict(title='G Total Loss'))\n",
    "                vis.line(np.array(Loss_gd_graph),win='gd',opts=dict(title='G-Discriminator Loss'))\n",
    "                vis.line(np.array(Loss_gf_graph),win='gf',opts=dict(title='G-Feature Loss'))\n",
    "                vis.line(np.array(Loss_d_graph),win='d',opts=dict(title='D Loss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "( 0 , 0 ,.,.) = \n",
       "  9.1897e-01  8.3165e-01  9.1564e-01  ...   6.6282e-01  8.8889e-01  7.3796e-01\n",
       "  9.0135e-01  8.6064e-01  9.1084e-01  ...   7.5758e-01  8.7981e-01  8.7815e-01\n",
       "  9.2900e-01  8.4735e-01  9.3579e-01  ...   8.4025e-01  9.5273e-01  9.0171e-01\n",
       "                 ...                   ⋱                   ...                \n",
       "  9.3104e-01  9.0913e-01  9.1532e-01  ...   9.2808e-01  9.0905e-01  9.1950e-01\n",
       "  9.2427e-01  8.8387e-01  9.2616e-01  ...   9.0912e-01  9.3567e-01  9.0101e-01\n",
       "  7.6688e-01  7.7039e-01  7.9515e-01  ...   8.5508e-01  7.9505e-01  7.6316e-01\n",
       "\n",
       "( 0 , 1 ,.,.) = \n",
       "  4.2017e-03  8.6683e-03 -5.6678e-02  ...   6.4134e-02  5.6974e-02  2.8360e-02\n",
       "  1.2334e-02  6.9427e-02  1.4359e-01  ...  -4.3871e-02  3.6610e-02 -6.5970e-02\n",
       "  1.6526e-01 -2.5374e-02 -4.7800e-02  ...  -1.4116e-01  2.5687e-02  1.5605e-02\n",
       "                 ...                   ⋱                   ...                \n",
       "  6.1316e-02  8.3448e-02  5.1586e-02  ...   1.0952e-01 -1.5752e-02 -1.2105e-01\n",
       "  1.8633e-01 -4.1360e-02 -5.3888e-02  ...  -9.7278e-02 -6.4623e-02 -9.3237e-02\n",
       " -3.9315e-02  1.9666e-02  4.5621e-02  ...  -2.2013e-02  5.5341e-02  1.1359e-02\n",
       "\n",
       "( 0 , 2 ,.,.) = \n",
       "  1.2488e-01  1.6684e-01  9.3380e-03  ...  -2.6090e-02 -3.7932e-02 -7.4237e-03\n",
       "  1.1149e-01  3.6875e-02  1.0260e-01  ...   1.1344e-01  5.2072e-02 -1.1682e-02\n",
       " -3.0450e-03  2.0000e-01  4.2840e-02  ...   6.9848e-02 -1.2195e-01  1.9502e-02\n",
       "                 ...                   ⋱                   ...                \n",
       " -2.1990e-02  6.9855e-03  8.2925e-02  ...   1.5263e-02 -9.3088e-02  2.4928e-02\n",
       " -4.2971e-02  8.5558e-02 -1.3199e-02  ...   3.0961e-02  6.9989e-02  1.4838e-01\n",
       " -1.3916e-01  6.9316e-02 -5.1876e-02  ...   1.1220e-01  1.9308e-02 -6.1368e-03\n",
       "      ⋮  \n",
       "\n",
       "( 1 , 0 ,.,.) = \n",
       "  9.1897e-01  8.3165e-01  9.1564e-01  ...   6.6282e-01  8.8889e-01  7.3796e-01\n",
       "  9.0135e-01  8.6064e-01  9.1084e-01  ...   7.5758e-01  8.7981e-01  8.7815e-01\n",
       "  9.2900e-01  8.4735e-01  9.3579e-01  ...   8.4025e-01  9.5273e-01  9.0171e-01\n",
       "                 ...                   ⋱                   ...                \n",
       "  9.3176e-01  9.0947e-01  9.1612e-01  ...   9.2713e-01  9.0886e-01  9.1827e-01\n",
       "  9.2479e-01  8.8458e-01  9.2685e-01  ...   9.0818e-01  9.3502e-01  9.0086e-01\n",
       "  7.6774e-01  7.7124e-01  7.9580e-01  ...   8.5490e-01  7.9553e-01  7.6255e-01\n",
       "\n",
       "( 1 , 1 ,.,.) = \n",
       "  4.2017e-03  8.6683e-03 -5.6679e-02  ...   6.4134e-02  5.6974e-02  2.8360e-02\n",
       "  1.2334e-02  6.9427e-02  1.4359e-01  ...  -4.3871e-02  3.6610e-02 -6.5970e-02\n",
       "  1.6526e-01 -2.5374e-02 -4.7800e-02  ...  -1.4116e-01  2.5687e-02  1.5605e-02\n",
       "                 ...                   ⋱                   ...                \n",
       "  6.1541e-02  8.3673e-02  5.1618e-02  ...   1.1259e-01 -1.3071e-02 -1.1845e-01\n",
       "  1.8611e-01 -4.1124e-02 -5.3529e-02  ...  -1.0137e-01 -6.6670e-02 -9.1281e-02\n",
       " -3.9425e-02  1.9436e-02  4.6060e-02  ...  -2.1288e-02  5.3631e-02  1.3476e-02\n",
       "\n",
       "( 1 , 2 ,.,.) = \n",
       "  1.2488e-01  1.6684e-01  9.3379e-03  ...  -2.6090e-02 -3.7932e-02 -7.4237e-03\n",
       "  1.1149e-01  3.6875e-02  1.0260e-01  ...   1.1344e-01  5.2072e-02 -1.1682e-02\n",
       " -3.0451e-03  2.0000e-01  4.2840e-02  ...   6.9848e-02 -1.2195e-01  1.9502e-02\n",
       "                 ...                   ⋱                   ...                \n",
       " -2.2851e-02  7.0999e-03  8.4118e-02  ...   1.2764e-02 -9.5448e-02  2.3978e-02\n",
       " -4.3728e-02  8.5236e-02 -1.3625e-02  ...   3.3249e-02  7.0454e-02  1.4557e-01\n",
       " -1.3921e-01  6.9696e-02 -5.1522e-02  ...   1.1286e-01  2.0762e-02 -4.1113e-03\n",
       "[torch.cuda.FloatTensor of size 2x3x256x256 (GPU 1)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "tensors are on different GPUs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-b3d643d97e14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlab_var\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mLLL\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeat_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLLL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/root/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda2/lib/python2.7/site-packages/torch/nn/modules/container.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda2/lib/python2.7/site-packages/torch/nn/modules/conv.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 254\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda2/lib/python2.7/site-packages/torch/nn/functional.pyc\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, weight, bias, stride, padding, dilation, groups)\u001b[0m\n\u001b[1;32m     50\u001b[0m     f = ConvNd(_pair(stride), _pair(padding), _pair(dilation), False,\n\u001b[1;32m     51\u001b[0m                _pair(0), groups, torch.backends.cudnn.benchmark, torch.backends.cudnn.enabled)\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: tensors are on different GPUs"
     ]
    }
   ],
   "source": [
    "#TODO normalize and minus mean\n",
    "feat_model=models.vgg19(pretrained=True)\n",
    "lab_var = fake\n",
    "L,A,B=torch.chunk(lab_var,3,dim=1)\n",
    "LLL=torch.cat((L,L,L),1)\n",
    "out=feat_model.features(LLL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
